[20s] Hi everyone! My name is Rui, and today, I’m happy to present our work on performing efficient flow scheduling in distributed deep learning training with echelon formation. I did this project during my internship at the Max Planck Institute for Informatics. ♦

[25s] Deep neural networks have evolved rapidly in the recent years. Models like ResNet, BERT, and GPT-3 have empowered state of the art results across a wide range of applications, including computer vision, natural language processing, etc. Before we deploy these DNNs in applications, however, they first need to be trained. ♦

[40s] DNNs are usually trained in a distributed manner in a cluster, ♦ where the workers use the network for communicating model weights and intermediate data. ♦ A trend in the recent years is that both the models and the datasets are getting bigger, and as a result, ♦ communication in distributed training is increasingly becoming a bottleneck. For example, in prior work, people have found that as much as 80% of the overall training time is spent on network communications. Therefore, improving the communication performance of distributed training is crucial to reducing the training time. ♦

[50s] A powerful tool to alleviate the communication bottleneck is flow scheduling, which will reduce the flow completion times and ultimately the overall completion time as well. ♦ CoFlow is the most relevant flow scheduling abstraction for data parallel applications to express their performance goals. ♦ It defines a set of semantically related flows that are optimized to finish at the same time. ♦ In the example of a MapReduce job, ♦ we can say that all flows in the shuffle phase constitutes a CoFlow, and the CoFlow will not finish until all its flows finish. ♦ Now, CoFlow is a really good fit for traditional big data frameworks, ♦ but unfortunately, it doesn’t work too well for some of the modern ML training paradigms. ♦

[30s] The reason why flow scheduling doesn’t apply to distributed training is that (you don’t need to read all of these) ♦ there have been a bunch of emerging strategies for parallelizing distributed training, ♦ and they have drastically different communication patterns, ♦ so it is challenging to come up with a network abstraction that unites them all. Next, let’s look at a concrete example of how existing flow scheduling abstractions fail for ML training. ♦

[70s] The distributed training paradigm we are showing here is called pipeline parallelism, which is a form of model parallelism. ♦ In this paradigm, the model is partitioned across multiple workers ♦ that each includes consecutive layers of the neural network, and each input sample passes through the workers sequentially. In this example, we have 4 workers. ♦ To maximize the system throughput, each training mini-batch is further split into micro-batches, so that computations can be executed as a pipeline, shown in this figure where we have 3 micro batches. Each iteration includes a forward phase and a backward phase. Note that this figure only shows the computations and omits communications for simplicity. ♦ So if we zoom in on the first two workers during the forward pass and plot the computation-communication dependency graph, we have something like this ♦. Each square block is the computation of a micro batch, each yellow circle denotes a network flow, and each arrow denotes a “starts-after” dependency. ♦

[60s] ♦ The problem with modeling these flows is that the communication patterns in distributed training are more sophisticated than those in traditional big data applications, and the flows may not finish at the same time. ♦ In the example of pipeline parallelism, the computations form a staggered pattern, so flows should similarly finish in a staggered manner. ♦ Suppose the bandwidth between these two workers is B, and each micro-batch needs to transmit 2B units of data. ♦ If we naively use CoFlow scheduling, ♦ all flows finish at the same time ♦ and it takes 10 units of time ♦ to finish this forward phase, ♦ which is even worse than bandwidth fair sharing. (pause) ♦ In the optimal case, we would maintain the staggered shape of the pipeline to ♦ achieve more computational efficiency ♦ and reduce the iteration time. ♦ The optimal case achieves better efficiency than CoFlow, and we do this with our abstraction, EchelonFlow. ♦

[100s] The insight of our EchelonFlow abstraction is that (pause) the finish times of flows should follow the computation pattern of distributed training, as communication is to facilitate the upcoming computation. ♦ We find this nature similar to echelon, a military terminology that describes the formation of units like planes or troops. ♦ An echelon formation is defined by ♦ the shape and distance between units. ♦ Similar to military units, ♦ computations in distributed training also form specific patterns, in which the shape is predetermined by the training paradigm that we know ahead of time, and the distance is the computation time on each worker, which can be profiled. For data transmission between workers, we define the ideal finish time as the earliest possible time that the flow can finish, like this ♦, which is usually the start time of the flow. By minimizing the real finish times with regard to the ideal finish times, ♦ we can make the flows finish as fast as possible, maintain the ideal arrangement pattern throughout the training process, and maximize the training efficiency. We repeat this process in each iteration of training. ♦ In case flows are delayed because of unexpected conditions in the network, ♦ we set the ideal finish times more aggressively, so that the upcoming flows receive more bandwidth allocation and catch up in order to reduce the deviation from the ideal computation-communication pattern. ♦

[20s] Formally, ♦ we define an EchelonFlow as a set of flows with related ideal finish times ♦ that are not necessarily equal, ♦ but their relationship can be represented by an arrangement function that describes how the flows should be arranged to maintain the ideal computation-communication pattern. ♦

[15s] EchelonFlow is very expressive, in the sense that it’s capable of modeling training paradigms that cannot be modeled with CoFlow. But for brevity, we are only gonna look at pipeline parallelism — please see our paper for more details on how we use EchelonFlow to model different parallelization strategies. ♦

[50s] The way we use EchelonFlow to model pipeline parallelism is as follows. Instead of using CoFlow to say that semantically related flows should finish at the same time, ♦ our abstraction tells the network when the first flow in the EchelonFlow should finish, and the “distances” between flows in this EchelonFlow. In pipeline parallelism, the distance T between flows is simply the time of one forward pass of one micro-batch. The formulation I showed here is the arrangement function, which describes the pattern formed by the ideal finish times of flows in the EchelonFlow. This is more powerful than CoFlow because we can describe more generic flow finish time patterns. ♦

[20s] As a quick recap, ♦ EchelonFlow is a generic abstraction for flow scheduling in diverse distributed training paradigms. ♦ It is a superset of CoFlow, which entails that CoFlow scheduling algorithms can be extended ♦ to EchelonFlow scheduling fairly easily, and they have the same complexity. ♦

[40s] A natural question is, how is flow scheduling for ml possible? ♦ Our abstraction assumes prior knowledge on the flow size, computation time, and computation-communication dependencies ♦ because deep learning training is a highly iterative process, ♦ and is predictable in the sense that the GPU computation time can be profiled, and the data dependencies and flow sizes are fixed across iterations. ♦ Thus, in an actual system, we can use profiling to obtain the computation time and data dependencies a priori. Please see our paper for a more detailed sketch of our envisioned system.

[35s] To conclude, ♦ EchelonFlow is a network abstraction ♦ for flow scheduling in diverse distributed training paradigms. ♦ Key to it is an arrangement function ♦ that expresses the communication patterns for higher training efficiency. ♦ Through this paper, we hope to encourage the community to do a better integration between machine learning systems and network management to regulate the network more proactively, instead of reactively. Thank you for listening to my talk, I’m happy to take questions.